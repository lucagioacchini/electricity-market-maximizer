{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36964bit783c28c53b0e437ca95db3440c52cd20",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Analysis\n",
    "Three models are developed and compared to determine which is the best one in terms of company strategy forecasting accuract.<br>\n",
    "The tested models are LSTM, ARIMA and SVR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from src.lstm import Correlation\n",
    "from matplotlib import rcParams \n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "from src.lstm import Preprocess\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Dropout, Bidirectional, TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import statsmodels.graphics.tsaplots as tplot\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from svr import Svr\n",
    "\n",
    "from src.arima import Arima\n",
    "from influxdb import InfluxDBClient\n",
    "from dateutil import relativedelta\n",
    "\n",
    "\n",
    "rcParams.update({'font.size': 11})\n",
    "rcParams.update({'figure.autolayout': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) LSTM\n",
    "### 1.1) Correlation Analysis\n",
    "\n",
    "The first step of the LSTM ananlysis is to detect which features are more informative.<br>\n",
    "Each MGP feature is correlated with the target variable and the ones with a correlation greater than 40% with the target one is saved, all the others are dropped."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT = .7\n",
    "TIMESTEPS = 7\n",
    "MIN_CORR = .4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Correlation(MIN_CORR)\n",
    "data, label = p.loadData()\n",
    "\n",
    "corr = p.selectFeatures(data, label)\n",
    "corr = corr.sort_values('Target', ascending=False)\n",
    "corr = corr.drop('Target')\n",
    "corr = corr.rename(\n",
    "    {\n",
    "        'DLY_QTY':'y(t)',\n",
    "        'MGP_CNOR_Prezzo':'C-North Price',\n",
    "        'MGP_AUST_Prezzo':'AT Price',\n",
    "        'MGP_BSP_Prezzo':'SVN Coupling Price',\n",
    "        'MGP_FRAN_Prezzo':'FR Price',\n",
    "        'MGP_NORD_Prezzo':'North Price',\n",
    "        'MGP_SLOV_Prezzo':'SVN Coupling Price',\n",
    "        'MGP_SVIZ_Prezzo':'CH Coupling Price',\n",
    "        'MGP_XAUS_Prezzo':'AT Coupling Price',\n",
    "        'MGP_XFRA_Prezzo':'FR Coupling Price',\n",
    "        'MGP_PUN_Prezzo':'National Price',\n",
    "        'MGP_NAT_Prezzo':'NAT Price',\n",
    "        'MGP_CSUD_Prezzo':'C-South Price',\n",
    "        'MGP_COAC_Prezzo':'Corsica Price',\n",
    "        'MGP_SARD_Prezzo':'Sardinia Coupling Price'\n",
    "    }\n",
    ")\n",
    "corr['Target']*=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.axhline(y=50, color='k', linestyle='-.', linewidth=1)\n",
    "plt.axhline(y=45, color='k', linestyle='-.', linewidth=1)\n",
    "plt.axhline(y=40, color='k', linestyle='-.', linewidth=1)\n",
    "plt.stem(corr.index,corr,  bottom=-2)\n",
    "plt.yticks([40,45,50,70,100])\n",
    "plt.ylim(30)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Correlation [%]')\n",
    "plt.xticks(rotation=90)\n",
    "plt.savefig('../fig/featuresCorrelation.png', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2) Model Training\n",
    "Now the model is developed. Two scenarios are investigated: <br>\n",
    "1. Next day forecasting <br>\n",
    "2. 8 days forecasting. In this second case the missing 7 days are forecasted and appended to the original dataset by considering them as observations<br>\n",
    "<br>\n",
    "Since from the correlation analysis there is only one feature with correlation greater than 50%, which is the time-shifted target variable, the LSTM model is developed to work only with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT = .7\n",
    "TIMESTEPS = 7\n",
    "MIN_CORR = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Preprocess(MIN_CORR)\n",
    "data , label= p.loadData()\n",
    "label = label.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the X and y datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = p.reshapeData(data)\n",
    "y = p.reshapeData(label)\n",
    "y = y.reshape(y.shape[0], y.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = p.scaleData(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(\n",
    "    LSTM(\n",
    "        200, \n",
    "        input_shape=(x_train.shape[1], x_train.shape[2]), \n",
    "        return_sequences=True, \n",
    "        unroll=True\n",
    "    )\n",
    ")\n",
    "model.add(Dropout(.2))\n",
    "\n",
    "model.add(\n",
    "    Bidirectional(\n",
    "        LSTM(\n",
    "            80, \n",
    "            input_shape=(x_train.shape[1], x_train.shape[2]),\n",
    "            unroll=True,\n",
    "            return_sequences=True, \n",
    "        )\n",
    "    )\n",
    ")\n",
    "model.add(Dropout(.2))\n",
    "\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "\n",
    "opt = Adam(learning_rate=0.0001)\n",
    "\n",
    "model.compile(loss='mse', optimizer=opt , metrics = ['mae', 'mape'])\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=1000, \n",
    "    batch_size=20, \n",
    "    validation_data=(x_test,y_test), \n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "model.save('../models/lstm.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot the loss function trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='Training', linewidth=2.5)\n",
    "plt.plot(\n",
    "    history.history['val_loss'], label='Test', linewidth=2.5, linestyle='-.', color = 'r'\n",
    ")\n",
    "plt.legend()\n",
    "plt.grid(linestyle = '--')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE')\n",
    "plt.savefig('../fig/loss.png', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Validation\n",
    "The entire dataset is forecasted and analyzed.\n",
    "\n",
    "#### Next Day Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reprocess(MIN_CORR)\n",
    "data , label= p.loadData()\n",
    "label = label.reshape(-1, 1)\n",
    "\n",
    "X = p.reshapeData(data)\n",
    "y = p.reshapeData(label)\n",
    "y = y.reshape(y.shape[0], y.shape[1])\n",
    "\n",
    "scaled = p.scaleData(X)\n",
    "model = load_model('../models/lstm.h5')\n",
    "\n",
    "y_hat = []\n",
    "for i in range(scaled.shape[0]):\n",
    "    y_hat.append(model.predict(scaled[i].reshape(1,scaled.shape[1], scaled.shape[2]))[0])\n",
    "\n",
    "y_hat = np.asarray(y_hat)\n",
    "y_hat = p.descaleData(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_save = {'y':y[:,-1],'y_hat':y_hat[:,-1]}\n",
    "pd.DataFrame(to_save).to_csv('../data/lstm1days.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8 Days Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Preprocess(MIN_CORR)\n",
    "original_data ,_= p.loadData()\n",
    "\n",
    "y_hat = []\n",
    "y = []\n",
    "model = load_model('../models/lstm.h5')\n",
    "for j in range(original_data.shape[0]):\n",
    "    # Emulate the next day\n",
    "    temp_data = original_data.copy()\n",
    "    try:\n",
    "        for i in range(8):\n",
    "            pred = []\n",
    "            # Load TIMESTEPS samples\n",
    "            data = temp_data.iloc[i+j:i+j+TIMESTEPS].copy()\n",
    "            # Create one single sample of 3D data\n",
    "            X = p.reshapeData(data)\n",
    "            X = X[0,:,:]\n",
    "            X = X.reshape(1,X.shape[0],X.shape[1])\n",
    "            # Scale Data\n",
    "            scaled = p.scaleData(X)\n",
    "            # Predict the next missing sample of Public Offers\n",
    "            pred.append(model.predict(scaled.reshape(1,scaled.shape[1], scaled.shape[2]))[0])\n",
    "            missing_off = p.descaleData(np.asarray(pred))[0,-1]\n",
    "            # If i is 7, the missing week has been\n",
    "            # forecasted, so the original sample is put in y, whereas\n",
    "            # the forecasted one is the target one (t+1)\n",
    "            if i == 7:\n",
    "                y.append(original_data.iloc[i+j+TIMESTEPS]['DLY_QTY'])\n",
    "                y_hat.append(missing_off)\n",
    "            # Replace the real data retrieved for training purpose with\n",
    "            # the forecasted sample.\n",
    "            temp_data.iloc[i+j+TIMESTEPS]['DLY_QTY'] = missing_off\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_save = {'y':y,'y_hat':y_hat}\n",
    "pd.DataFrame(to_save).to_csv('../data/lstm8days.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) ARIMA\n",
    "The first step consists on analyzing the timeseries stationarity, since the ARIMA model can be applied only to stationary timeseries. Both a analytical method (Augmented Dickey Fuller test) and a graphical one based on the ACF and PACF analysis are used.<br>\n",
    "Then, in case of stationarity, a grid search is performed to determine the best ARIMA hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dickeyFuller(data, model):\n",
    "    cutoff=0.00001\n",
    "    # Perform Dickey-Fuller test:\n",
    "    print(\n",
    "        f'Dickey-Fuller Test {model}:'\n",
    "    )\n",
    "    p_val = adfuller(data['DLY_QTY'].dropna(), autolag='AIC')[1]\n",
    "    if p_val < cutoff:\n",
    "        print(f'\\tp-value = {round(p_val,2)}. Stationary.')\n",
    "    else:\n",
    "        print(f'\\tp-value = {round(p_val,2)}. Non-stationary.')\n",
    "\n",
    "\n",
    "def statTest(data):\n",
    "    # 0 order differentiation\n",
    "    rolling_mean = data.rolling(window = 24).mean()\n",
    "    rolling_std = data.rolling(window = 24).std()\n",
    "    dickeyFuller(data, '0d')\n",
    "    # 1 order differentiation\n",
    "    rolling_mean_diff = (\n",
    "        data\n",
    "        .diff()\n",
    "        .rolling(window = 24)\n",
    "        .mean()\n",
    "    )\n",
    "    rolling_std_diff = (\n",
    "        data\n",
    "        .diff()\n",
    "        .rolling(window = 24)\n",
    "        .std()\n",
    "    )\n",
    "    dickeyFuller(data.diff(), '1d')\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure()\n",
    "    plt.plot(data, label='Original Timeseries')\n",
    "    plt.plot(rolling_mean, label='Rolling mean', color='r')\n",
    "    plt.plot(rolling_std, label='Rolling std', color='k')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Offered Daily Quantity')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.grid()\n",
    "    plt.legend(ncol=3, loc='upper center', bbox_to_anchor=(0.5, 1.30))\n",
    "    plt.savefig('../fig/rollingD0.png', transparent=True)\n",
    "    plt.close()\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure()\n",
    "    plt.plot(data.diff(), label='Differenced Timeseries')\n",
    "    plt.plot(rolling_mean_diff, label='Rolling mean', color='r')\n",
    "    plt.plot(rolling_std_diff, label='Rolling std', color='k')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Offered Daily Quantity')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.grid()\n",
    "    plt.legend(ncol=3, loc='upper center', bbox_to_anchor=(0.5, 1.30))\n",
    "    plt.savefig('../fig/rollingD1.png', transparent=True)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Stationarity Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLIM = 6\n",
    "QLIM = 4\n",
    "\n",
    "WINDOW = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "off = (\n",
    "    pd\n",
    "    .read_csv('../data/bid.csv', index_col='Unnamed: 0')\n",
    "    .drop(\n",
    "        columns = [\n",
    "            'DLY_AWD_QTY',\n",
    "            'TYPE', \n",
    "            'DLY_PRICE', \n",
    "            'DLY_AWD_PRICE'\n",
    "        ]\n",
    "    )\n",
    "    .dropna()\n",
    ")\n",
    "df = off \n",
    "df = df.set_index(pd.DatetimeIndex(off.index))\n",
    "df = df.resample('D').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the Dickey Fuller test and check if the 1st order differentiation improves the performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statTest(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the ACF and PACF analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df\n",
    "fig, ax = plt.subplots(2, 1, sharex=True, figsize=(15,10))\n",
    "tplot.plot_acf(\n",
    "    data, \n",
    "    ax[0],\n",
    "    lags=40, \n",
    "    markersize=0,\n",
    "    title='Original'\n",
    ")\n",
    "tplot.plot_acf(\n",
    "    data.diff().dropna(), \n",
    "    ax[1],\n",
    "    lags=40, \n",
    "    markersize=0,\n",
    "    title='1st Order Differentiation'\n",
    ")\n",
    "\n",
    "plt.savefig('../fig/acf.png', transparent=True)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df\n",
    "fig, ax = plt.subplots(2, 1, sharex=True, figsize=(15,10))\n",
    "tplot.plot_pacf(\n",
    "    data, \n",
    "    ax[0],\n",
    "    lags=40, \n",
    "    markersize=0,\n",
    "    title='Original'\n",
    ")\n",
    "tplot.plot_pacf(\n",
    "    data.diff().dropna(), \n",
    "    ax[1],\n",
    "    lags=40, \n",
    "    markersize=0,\n",
    "    title='1st Order Differentiation'\n",
    ")\n",
    "\n",
    "plt.savefig('../fig/pacf.png', transparent=True)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Grid Search\n",
    "\n",
    "#### 2.2.1) Next Day Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 1\n",
    "\n",
    "X = df.values\n",
    "X = X[1:]\n",
    "\n",
    "p_val = np.arange(0,PLIM)\n",
    "q_val = np.arange(0,QLIM)\n",
    "\n",
    "mse = np.zeros((len(p_val), len(q_val)))\n",
    "mape = np.zeros((len(p_val), len(q_val)))\n",
    "for p in p_val:\n",
    "    for q in q_val:\n",
    "        y = []\n",
    "        y_hat = []\n",
    "        print(f'ARIMA({p},1,{q})')\n",
    "        for i in range(X.shape[0]):\n",
    "            try:\n",
    "                train = X[i:i+WINDOW]\n",
    "\n",
    "                model = ARIMA(\n",
    "                    train, \n",
    "                    order=(p,1,q),\n",
    "                )\n",
    "                model_fit = model.fit(maxiter=200, disp=0, method='css')\n",
    "                y.append(X[i+WINDOW-1])\n",
    "                y_hat.append(model_fit.forecast(h)[0][-1])\n",
    "            except:\n",
    "                pass\n",
    "        mse[p_val[p]][q_val[q]] = mean_squared_error(y, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the MSE Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,8))\n",
    "ax = sns.heatmap(mse, annot=True, fmt='.1f', cmap='YlGnBu', square=True, cbar_kws={'label':'MSE'}, vmin=0)\n",
    "ax.set_ylim(PLIM,0)\n",
    "ax.set_xlim(QLIM,0)\n",
    "ax.invert_yaxis()\n",
    "ax.invert_xaxis()\n",
    "ax.set_ylabel('Autoregressive Order (p)')\n",
    "ax.set_xlabel('Moving Average Order (q)')\n",
    "plt.savefig('../fig/heatmap1day.png', transparent=True)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2) 8 Day Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 8\n",
    "\n",
    "X = df.values\n",
    "X = X[1:]\n",
    "\n",
    "p_val = np.arange(0,PLIM)\n",
    "q_val = np.arange(0,QLIM)\n",
    "\n",
    "mse = np.zeros((len(p_val), len(q_val)))\n",
    "mape = np.zeros((len(p_val), len(q_val)))\n",
    "for p in p_val:\n",
    "    for q in q_val:\n",
    "        y = []\n",
    "        y_hat = []\n",
    "        print(f'ARIMA({p},1,{q})')\n",
    "        for i in range(X.shape[0]):\n",
    "            try:\n",
    "                train = X[i:i+WINDOW]\n",
    "\n",
    "                model = ARIMA(\n",
    "                    train, \n",
    "                    order=(p,1,q),\n",
    "                )\n",
    "                model_fit = model.fit(maxiter=200, disp=0, method='css')\n",
    "                y.append(X[i+WINDOW-1])\n",
    "                y_hat.append(model_fit.forecast(h)[0][-1])\n",
    "            except:\n",
    "                pass\n",
    "        mse[p_val[p]][q_val[q]] = mean_squared_error(y, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the MSE Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,8))\n",
    "ax = sns.heatmap(mse, annot=True, fmt='.1f', cmap='YlGnBu', square=True, cbar_kws={'label':'MSE'}, vmin=0)\n",
    "ax.set_ylim(PLIM,0)\n",
    "ax.set_xlim(QLIM,0)\n",
    "ax.invert_yaxis()\n",
    "ax.invert_xaxis()\n",
    "ax.set_ylabel('Autoregressive Order (p)')\n",
    "ax.set_xlabel('Moving Average Order (q)')\n",
    "plt.savefig('fig/heatmap8day.png', transparent=True)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3) Model Implementation\n",
    "Since the ARIMA(0,1,0) leads to the best performances, the final model has been implemented and then the entire dataset is investigated to check the forecasting accuracy.<br>\n",
    "Two scenarios are investigated: <br>\n",
    "1. Next day forecasting <br>\n",
    "2. 8 days forecasting <br>\n",
    "The observation window size is fixed to 60 past days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and perform a daily resampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "off = (\n",
    "    pd\n",
    "    .read_csv('../data/bid.csv', index_col='Unnamed: 0')\n",
    "    .drop(\n",
    "        columns = [\n",
    "            'DLY_AWD_QTY',\n",
    "            'TYPE', \n",
    "            'DLY_PRICE', \n",
    "            'DLY_AWD_PRICE'\n",
    "        ]\n",
    "    )\n",
    "    .dropna()\n",
    ")\n",
    "df = off \n",
    "df = df.set_index(pd.DatetimeIndex(off.index))\n",
    "df = df.resample('D').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1) Next Day Validation\n",
    "The horizon is set equal to 1 and all the dataset is iteratively<br>\n",
    "analyzed by making the window sliding from one day to the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 1\n",
    "\n",
    "X = df.values\n",
    "X = X[1:]\n",
    "\n",
    "y = []\n",
    "y_hat = []\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    try:\n",
    "        train = np.copy(X[i:i+WINDOW])\n",
    "\n",
    "        model = ARIMA(\n",
    "            train, \n",
    "            order=(0,1,0),\n",
    "        )\n",
    "        model_fit = model.fit(maxiter=200, disp=0, method='css')\n",
    "        y.append(X[i+WINDOW-1])\n",
    "        y_hat.append(model_fit.forecast(h)[0][-1])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the result in a .csv file which will be used <br>\n",
    "in the models comparison phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_save = {'y':y, 'y_hat':y_hat}\n",
    "pd.DataFrame(to_save).to_csv('../data/arima1days.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2) 8 Days Validation\n",
    "The horizon is set equal to 8, this means tht at each timestep, <br>\n",
    "the next eighth day is forecasted. All the dataset is iteratively<br>\n",
    "analyzed by making the window sliding from one day to the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 8\n",
    "\n",
    "X = df.values\n",
    "X = X[1:]\n",
    "\n",
    "y = []\n",
    "y_hat = []\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    try:\n",
    "        train = np.copy(X[i:i+WINDOW])\n",
    "\n",
    "        model = ARIMA(\n",
    "            train, \n",
    "            order=(0,1,0),\n",
    "        )\n",
    "        model_fit = model.fit(maxiter=200, disp=0, method='css')\n",
    "        y.append(X[i+WINDOW-1])\n",
    "        y_hat.append(model_fit.forecast(h)[0][-1])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the result in a .csv file which will be used <br>\n",
    "in the models comparison phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_save = {'y':y,'y_hat':y_hat}\n",
    "pd.DataFrame(to_save).to_csv('../data/arima8days.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) SVR\n",
    "A grid search is performed to detect the best observation window. Two scenarios are investigated: <br>\n",
    "1. Next day forecasting <br>\n",
    "2. 8 days forecasting. In this second case the missing 7 days are forecasted and appended to the original dataset by considering them as observations<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams.update({'font.size': 14})\n",
    "\n",
    "MIN_CORR = .1\n",
    "N_WINDOW = 62"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1) Grid Search\n",
    "\n",
    "#### 3.1.1) Next Day Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Svr()\n",
    "original_data, _= p.loadData()\n",
    "N=1\n",
    "MSE = []\n",
    "for WINDOW in np.arange(2, N_WINDOW):\n",
    "    print(f'\\tWindow size:{WINDOW}')\n",
    "    y = []\n",
    "    y_hat = []\n",
    "    for j in range(original_data.shape[0]):\n",
    "        # Emulate the next day\n",
    "        temp_data = original_data.copy()\n",
    "        try:\n",
    "            for i in range(N):\n",
    "                pred = []\n",
    "                # Load the last available sample\n",
    "                X = temp_data.values[j+i:i+j+WINDOW+1].reshape(WINDOW+1,-1)\n",
    "                target = temp_data.values[i+j+1:i+j+WINDOW+2, -1].reshape(WINDOW+1,-1)\n",
    "                # Scale Data\n",
    "                x_train, y_train, x_test, y_test = p.scaleData(WINDOW, X, target)\n",
    "                model = SVR(kernel='rbf',gamma='auto',C=1.0,epsilon=0.1)\n",
    "                model.fit(x_train, y_train[:,0])\n",
    "                # Predict the next missing sample of Public Offers\n",
    "                pred.append(model.predict(x_test))\n",
    "                missing_off = p.descaleData(np.asarray(pred))[0,0]\n",
    "                # If i is 7, the missing week has been\n",
    "                # forecasted, so the original sample is put in y, whereas\n",
    "                # the forecasted one is the target one (t+1)\n",
    "                if i == N-1:\n",
    "                    y.append(p.descaleData(np.asarray(y_test))[0,0])\n",
    "                    y_hat.append(missing_off)\n",
    "                # Replace the real data retrieved for training purpose with\n",
    "                # the forecasted sample.\n",
    "                temp_data.iloc[i+j+1]['DLY_QTY'] = missing_off\n",
    "        except:\n",
    "            pass\n",
    "    # Compute metrics\n",
    "    mse = mean_squared_error(y, y_hat)\n",
    "    MSE.append(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.xlabel('Window Size')\n",
    "plt.ylabel('MSE')\n",
    "plt.plot(np.arange(2, N_WINDOW), MSE, color='b', linestyle='-.',label='MSE')\n",
    "plt.grid(linestyle='-.')\n",
    "plt.savefig('../fig/svrSearch1d.png', transparent=True)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8 Days Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Svr()\n",
    "original_data, _= p.loadData()\n",
    "N=8\n",
    "MSE = []\n",
    "for WINDOW in np.arange(2, N_WINDOW):\n",
    "    print(f'\\tWindow size:{WINDOW}')\n",
    "    y = []\n",
    "    y_hat = []\n",
    "    for j in range(original_data.shape[0]):\n",
    "        # Emulate the next day\n",
    "        temp_data = original_data.copy()\n",
    "        try:\n",
    "            for i in range(N):\n",
    "                pred = []\n",
    "                # Load the last available sample\n",
    "                X = temp_data.values[j+i:i+j+WINDOW+1].reshape(WINDOW+1,-1)\n",
    "                target = temp_data.values[i+j+1:i+j+WINDOW+2, -1].reshape(WINDOW+1,-1)\n",
    "                # Scale Data\n",
    "                x_train, y_train, x_test, y_test = p.scaleData(WINDOW, X, target)\n",
    "                model = SVR(kernel='rbf',gamma='auto',C=1.0,epsilon=0.1)\n",
    "                model.fit(x_train, y_train[:,0])\n",
    "                # Predict the next missing sample of Public Offers\n",
    "                pred.append(model.predict(x_test))\n",
    "                missing_off = p.descaleData(np.asarray(pred))[0,0]\n",
    "                # If i is 7, the missing week has been\n",
    "                # forecasted, so the original sample is put in y, whereas\n",
    "                # the forecasted one is the target one (t+1)\n",
    "                if i == N-1:\n",
    "                    y.append(p.descaleData(np.asarray(y_test))[0,0])\n",
    "                    y_hat.append(missing_off)\n",
    "                # Replace the real data retrieved for training purpose with\n",
    "                # the forecasted sample.\n",
    "                temp_data.iloc[i+j+1]['DLY_QTY'] = missing_off\n",
    "        except:\n",
    "            pass\n",
    "    # Compute metrics\n",
    "    mse = mean_squared_error(y, y_hat)\n",
    "    MSE.append(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.grid(linestyle='-.')\n",
    "plt.xlabel('Window Size')\n",
    "plt.ylabel('MSE')\n",
    "plt.plot(np.arange(2, N_WINDOW), MSE, color='b', linestyle='-.',label='MSE')\n",
    "plt.savefig('../fig/svrSearch8d.png', transparent=True)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) Model Validation\n",
    "The entire dataset is investigated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams.update({'font.size': 11})\n",
    "TIMESTEPS = 7\n",
    "WINDOW = 7\n",
    "MIN_CORR = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next Day Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Svr()\n",
    "original_data, _= p.loadData()\n",
    "\n",
    "N=1\n",
    "y = []\n",
    "y_hat = []\n",
    "for j in range(original_data.shape[0]):\n",
    "    # Emulate the next day\n",
    "    temp_data = original_data.copy()\n",
    "    try:\n",
    "        for i in range(N):\n",
    "            pred = []\n",
    "            # Load the last available sample\n",
    "            X = temp_data.values[j+i:i+j+WINDOW+1].reshape(WINDOW+1,-1)\n",
    "            target = temp_data.values[i+j+1:i+j+WINDOW+2, -1].reshape(WINDOW+1,-1)\n",
    "            # Scale Data\n",
    "            x_train, y_train, x_test, y_test = p.scaleData(WINDOW, X, target)\n",
    "            model = SVR(kernel='rbf',gamma='auto',C=1.0,epsilon=0.1)\n",
    "            model.fit(x_train, y_train[:,0])\n",
    "            # Predict the next missing sample of Public Offers\n",
    "            pred.append(model.predict(x_test))\n",
    "            missing_off = p.descaleData(np.asarray(pred))[0,0]\n",
    "            # If i is 7, the missing week has been\n",
    "            # forecasted, so the original sample is put in y, whereas\n",
    "            # the forecasted one is the target one (t+1)\n",
    "            if i == N-1:\n",
    "                y.append(p.descaleData(np.asarray(y_test))[0,0])\n",
    "                y_hat.append(missing_off)\n",
    "            # Replace the real data retrieved for training purpose with\n",
    "            # the forecasted sample.\n",
    "            temp_data.iloc[i+j+1]['DLY_QTY'] = missing_off\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_save = {\n",
    "    'y':y,\n",
    "    'y_hat':y_hat\n",
    "}\n",
    "pd.DataFrame(to_save).to_csv('../data/svr1days.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8 Days Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Svr()\n",
    "original_data, _= p.loadData()\n",
    "\n",
    "N=8\n",
    "y = []\n",
    "y_hat = []\n",
    "for j in range(original_data.shape[0]):\n",
    "    # Emulate the next day\n",
    "    temp_data = original_data.copy()\n",
    "    try:\n",
    "        for i in range(N):\n",
    "            pred = []\n",
    "            # Load the last available sample\n",
    "            X = temp_data.values[j+i:i+j+WINDOW+1].reshape(WINDOW+1,-1)\n",
    "            target = temp_data.values[i+j+1:i+j+WINDOW+2, -1].reshape(WINDOW+1,-1)\n",
    "            # Scale Data\n",
    "            x_train, y_train, x_test, y_test = p.scaleData(WINDOW, X, target)\n",
    "            model = SVR(kernel='rbf',gamma='auto',C=1.0,epsilon=0.1)\n",
    "            model.fit(x_train, y_train[:,0])\n",
    "            # Predict the next missing sample of Public Offers\n",
    "            pred.append(model.predict(x_test))\n",
    "            missing_off = p.descaleData(np.asarray(pred))[0,0]\n",
    "            # If i is 7, the missing week has been\n",
    "            # forecasted, so the original sample is put in y, whereas\n",
    "            # the forecasted one is the target one (t+1)\n",
    "            if i == N-1:\n",
    "                y.append(p.descaleData(np.asarray(y_test))[0,0])\n",
    "                y_hat.append(missing_off)\n",
    "            # Replace the real data retrieved for training purpose with\n",
    "            # the forecasted sample.\n",
    "            temp_data.iloc[i+j+1]['DLY_QTY'] = missing_off\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_save = {\n",
    "    'y':y,\n",
    "    'y_hat':y_hat\n",
    "}\n",
    "pd.DataFrame(to_save).to_csv('../data/svr8days.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Models Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1) 8 Days Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima = pd.read_csv('../data/arima8days.csv')\n",
    "lstm = pd.read_csv('../data/lstm8days.csv')\n",
    "svr = pd.read_csv('../data/svr8days.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trim datasets to make them comparable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima = arima.drop(988)\n",
    "lstm = lstm.drop(np.arange(0,46))\n",
    "svr = svr.drop(np.arange(0,45))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Line plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lstm.y.values[530:561], label='Observation', marker='o', color='C0', markersize=4)\n",
    "plt.plot(lstm.y_hat.values[530:561], label='LSTM', marker='*', linestyle='-.', color='C1', markersize=4)\n",
    "plt.plot(arima.y_hat.values[530:561], label='ARIMA(0,1,0)', marker='^', linestyle='--', color='C2', markersize=4)\n",
    "plt.plot(svr.y_hat.values[530:561], label='SVR', marker='s', linestyle=':', color='C3', markersize=4)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Daily Quantity [MWh]')\n",
    "plt.grid('--')\n",
    "plt.xlim(0)\n",
    "plt.savefig('../fig/comparison8days.png', transparent=True)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(lstm.y.values, lstm.y_hat.values, color='C3', s=10, alpha=.8, label='LSTM',marker='o')\n",
    "plt.scatter(lstm.y.values, arima.y_hat.values, color='C1', s=9, alpha=.8, label='ARIMA(0,1,0)',marker='s')\n",
    "plt.scatter(lstm.y.values, svr.y_hat.values, color='darkgreen', s=10, alpha=.8, label='SVR',marker='^')\n",
    "plt.plot(lstm.y.values, lstm.y.values, color='r', linewidth=2)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Observations [MWh]')\n",
    "plt.ylabel('Predictions [MWh]')\n",
    "plt.grid('--')\n",
    "plt.xlim(0)\n",
    "plt.savefig('../fig/scatter8days.png', transparent=True)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARIMA metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(lstm.y.values, arima.y_hat.values)\n",
    "mape = np.mean(np.abs((lstm.y.values - arima.y_hat.values) / lstm.y.values)) * 100\n",
    "mae = mean_absolute_error(lstm.y.values, arima.y_hat.values)\n",
    "r2 = r2_score(lstm.y.values, arima.y_hat.values)\n",
    "print(f'\\tMSE: {round(mse,2)}')\n",
    "print(f'\\tMAE: {round(mae,2)}')\n",
    "print(f'\\tMAPE: {round(mape,2)}%')\n",
    "print(f'\\tR2: {round(r2,2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(lstm.y.values, lstm.y_hat.values)\n",
    "mape = np.mean(np.abs((lstm.y.values - lstm.y_hat.values) / lstm.y.values)) * 100\n",
    "mae = mean_absolute_error(lstm.y.values, lstm.y_hat.values)\n",
    "r2 = r2_score(lstm.y.values, lstm.y_hat.values)\n",
    "print(f'\\tMSE: {round(mse,2)}')\n",
    "print(f'\\tMAE: {round(mae,2)}')\n",
    "print(f'\\tMAPE: {round(mape,2)}%')\n",
    "print(f'\\tR2: {round(r2,2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVR metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(lstm.y.values, svr.y_hat.values)\n",
    "mape = np.mean(np.abs((lstm.y.values - svr.y_hat.values) / lstm.y.values)) * 100\n",
    "mae = mean_absolute_error(lstm.y.values, svr.y_hat.values)\n",
    "r2 = r2_score(lstm.y.values, svr.y_hat.values)\n",
    "print(f'\\tMSE: {round(mse,2)}')\n",
    "print(f'\\tMAE: {round(mae,2)}')\n",
    "print(f'\\tMAPE: {round(mape,2)}%')\n",
    "print(f'\\tR2: {round(r2,2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2) Next Day Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima = pd.read_csv('../data/arima1days.csv')\n",
    "lstm = pd.read_csv('../data/lstm1days.csv')\n",
    "svr = pd.read_csv('../data/svr1days.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trim datasets to make them comparable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima = arima.drop(988)\n",
    "lstm = lstm.drop(np.arange(1041,1048))\n",
    "lstm = lstm.drop(np.arange(0,53))\n",
    "svr = svr.drop(np.arange(0,52))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Line plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lstm.y.values[530:561], label='Observation', marker='o', color='C0', markersize=4)\n",
    "plt.plot(lstm.y_hat.values[530:561], label='LSTM', marker='*', linestyle='-.', color='C1', markersize=4)\n",
    "plt.plot(arima.y_hat.values[530:561], label='ARIMA(0,1,0)', marker='^', linestyle='--', color='C2', markersize=4)\n",
    "plt.plot(svr.y_hat.values[530:561], label='SVR', marker='s', linestyle=':', color='C3', markersize=4)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Daily Quantity [MWh]')\n",
    "plt.grid('--')\n",
    "plt.ylim(0, 20000)\n",
    "plt.xlim(0)\n",
    "plt.savefig('../fig/comparison1days.png', transparent=True)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(lstm.y.values, lstm.y_hat.values, color='C3', s=10, alpha=.8, label='LSTM',marker='o')\n",
    "plt.scatter(lstm.y.values, arima.y_hat.values, color='C1', s=9, alpha=.8, label='ARIMA(0,1,0)',marker='s')\n",
    "plt.scatter(lstm.y.values, svr.y_hat.values, color='darkgreen', s=10, alpha=.8, label='SVR',marker='^')\n",
    "plt.plot(lstm.y.values, lstm.y.values, color='r', linewidth=2)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Observations [MWh]')\n",
    "plt.ylabel('Predictions [MWh]')\n",
    "plt.grid('--')\n",
    "plt.xlim(0)\n",
    "plt.savefig('../fig/scatter1days.png', transparent=True)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARIMA metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(lstm.y.values, arima.y_hat.values)\n",
    "mape = np.mean(np.abs((lstm.y.values - arima.y_hat.values) / lstm.y.values)) * 100\n",
    "mae = mean_absolute_error(lstm.y.values, arima.y_hat.values)\n",
    "r2 = r2_score(lstm.y.values, arima.y_hat.values)\n",
    "print(f'\\tMSE: {round(mse,2)}')\n",
    "print(f'\\tMAE: {round(mae,2)}')\n",
    "print(f'\\tMAPE: {round(mape,2)}%')\n",
    "print(f'\\tR2: {round(r2,2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(lstm.y.values, lstm.y_hat.values)\n",
    "mape = np.mean(np.abs((lstm.y.values - lstm.y_hat.values) / lstm.y.values)) * 100\n",
    "mae = mean_absolute_error(lstm.y.values, lstm.y_hat.values)\n",
    "r2 = r2_score(lstm.y.values, lstm.y_hat.values)\n",
    "print(f'\\tMSE: {round(mse,2)}')\n",
    "print(f'\\tMAE: {round(mae,2)}')\n",
    "print(f'\\tMAPE: {round(mape,2)}%')\n",
    "print(f'\\tR2: {round(r2,2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVR metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(lstm.y.values, svr.y_hat.values)\n",
    "mape = np.mean(np.abs((lstm.y.values - svr.y_hat.values) / lstm.y.values)) * 100\n",
    "mae = mean_absolute_error(lstm.y.values, svr.y_hat.values)\n",
    "r2 = r2_score(lstm.y.values, svr.y_hat.values)\n",
    "print(f'\\tMSE: {round(mse,2)}')\n",
    "print(f'\\tMAE: {round(mae,2)}')\n",
    "print(f'\\tMAPE: {round(mape,2)}%')\n",
    "print(f'\\tR2: {round(r2,2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Distribution of ARIMA Forecasting Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = InfluxDBClient('localhost', 8086, 'root', 'root', 'PublicBids')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the active operators. Assume that today is the 2/03/2020.<br>\n",
    "ARIMA works with 60 days as history, so it starts from the 2/01/2020.<br>\n",
    "The predicted values are referred to the 3/03/2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TODAY = datetime.strptime('15/03/2020', '%d/%m/%Y')\n",
    "START = TODAY - relativedelta.relativedelta(days=60)\n",
    "START = int(datetime.timestamp(START)*1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OP_LIST = []\n",
    "for market in ['MGP', 'MI', 'MSD']:\n",
    "    res = client.query(f\"SELECT * FROM demand{market} WHERE time >= {START}\").raw\n",
    "    for val in res['series'][0]['values']:\n",
    "        if val[3] not in OP_LIST and \"'\" not in val[3]:\n",
    "            OP_LIST.append(val[3])\n",
    "\n",
    "    res = client.query(f\"SELECT * FROM supply{market} WHERE time >= {START}\").raw\n",
    "    for val in res['series'][0]['values']:\n",
    "        if val[3] not in OP_LIST and \"'\" not in val[3]:\n",
    "            OP_LIST.append(val[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the next day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = []\n",
    "y = []\n",
    "\n",
    "TODAY = datetime.strptime('2/03/2020', '%d/%m/%Y')\n",
    "for op in OP_LIST:\n",
    "    if \"'\" not in op:\n",
    "        arima = Arima(op, TODAY)\n",
    "        pred, val = arima.predict()\n",
    "        y_hat += pred\n",
    "        y += val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the absolute point-wise error and visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errs = []\n",
    "for i in range(len(y)):\n",
    "    errs.append(\n",
    "        np.abs((y[i] - y_hat[i]))\n",
    "    )\n",
    "errs.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 7))\n",
    "plt.plot(errs, label=TODAY.strftime('%d/%m/%Y'))\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel('Number of predictions')\n",
    "plt.ylabel('Absolute point-wise error')\n",
    "plt.savefig('../fig/arimaErr.png', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge everything and repeat for some days to obtain a performance overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TODAY = datetime.strptime('2/03/2020', '%d/%m/%Y')\n",
    "plt.figure(figsize=(9, 7))\n",
    "\n",
    "for i in range(5):\n",
    "    y_hat = []\n",
    "    y = []\n",
    "\n",
    "    date = TODAY + relativedelta.relativedelta(days=i)\n",
    "    for op in OP_LIST:\n",
    "        if \"'\" not in op:\n",
    "            arima = Arima(op, date)\n",
    "            pred, val = arima.predict()\n",
    "            y_hat += pred\n",
    "            y += val\n",
    "\n",
    "    errs = []\n",
    "    for i in range(len(y)):\n",
    "        errs.append(\n",
    "            np.abs((y[i] - y_hat[i]))\n",
    "        )\n",
    "    errs.sort()\n",
    "\n",
    "    plt.plot(errs, label=date.strftime('%d/%m/%Y'))\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel('Number of predictions')\n",
    "plt.ylabel('Absolute point-wise error')\n",
    "plt.savefig('../fig/arimaErrComp.png', transparent=True)"
   ]
  }
 ]
}